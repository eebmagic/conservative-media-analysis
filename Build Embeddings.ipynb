{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce498b90",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7bfd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ebolton/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ebolton/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataProcessing import getTextsInDateRange, getPairs, getFullWordset, buildTokenizer\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0efd9",
   "metadata": {},
   "source": [
    "# Conda env checking\n",
    "From article here:\n",
    "https://towardsdatascience.com/get-your-conda-environment-to-show-in-jupyter-notebooks-the-easy-way-17010b76e874\n",
    "\n",
    "If conda env is being used correctly, the environment variable below should be `base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce9feb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486e10d",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b56e7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNEL_MAPPINGS = {\n",
    "#     \"Daily Wire\": \"UCroKPvbmaQKGK5tjtQsvaDw\",\n",
    "    \"Ben Shapiro\": \"UCnQC_G5Xsjhp9fEJKuIcrSw\",\n",
    "#     \"Daily Wire Plus\": \"UCaeO5vkdj5xOQHp4UmIN6dw\",\n",
    "#     \"Matt Walsh\": \"UCO01ytfzgXYy4glnPJm4PPQ\",\n",
    "#     \"Michael Knowls\": \"UCr4kgAUTFkGIwlWSodg43QA\",\n",
    "#     \"Candace Owens\": \"UCkY4fdKOFk3Kiq7g5LLKYLw\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139ba75",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a5afd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels = ['Ben Shapiro']\n",
    "channels = list(CHANNEL_MAPPINGS.keys())\n",
    "start = 2021\n",
    "stop = 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661dfc2",
   "metadata": {},
   "source": [
    "# Get Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390fe59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/ebolton/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sad', 'left', 'california', 'tell', 'thought', 'one', 'day', 'decision', 'one', 'best', 'decision', 'ever', 'made', 'family', 'know', 'orthodox', 'jewish', 'faith', 'mean', 'move', 'place', 'significant', 'jewish', 'resource', 'inundation', 'homelessness', 'area', 'like', 'kid', 'could'], ['bizarre', 'new', 'world', 'created', 'counter-reality', 'new', 'world', \"we've\", 'created', 'assigning', 'sex', 'baby', 'dc', 'forcing', 'label', 'mother', \"that's\", 'case', 'throne', 'mother', 'whole', 'birth', 'certificate', 'thing', 'really', 'really', 'causing', 'lot', 'oh', 'i'], ['end', 'entire', 'group', 'people', 'single', 'woman', 'falling', 'behind', 'married', 'woman', 'engaged', 'social', 'structure', 'important', 'mediating', 'institution', 'human', 'existence', 'marriage', 'ripped', 'away', \"they've\", 'told', 'longer', 'essential', 'important', 'fact', 'many', 'way', 'institution'], ['idea', 'left', 'everybody', 'right', 'keep', 'saying', \"there's\", 'election', 'fraud', \"that's\", 'reason', \"they're\", 'losing', 'election', 'left', \"there's\", 'dual', 'narrative', 'one', 'voter', 'suppression', 'exist', 'two', 'idea', 'must', 'disinformation', 'lose', 'well', 'also', 'explains'], ['true', 'people', 'talk', 'thing', 'want', 'talked', \"that's\", 'true', 'either', 'wanted', 'cancel', 'malcolm', \"i'm\", 'sorry', 'lying', \"that's\", 'fine', 'lying', \"i've\", 'personally', 'talked', 'many', 'thing', 'mentioned', 'define', 'critical', 'critical', 'race', 'theory', 'citing'], ['i', 'think', 'capitalism', 'stupid', 'work', 'i', 'mean', 'obviously', 'look', 'country', 'work', 'okay', 'pause', 'one', 'second', 'favorite', 'people', 'wearing', 'thing', 'bought', 'store', 'much', 'nicer', 'anything', 'could', 'get', 'communist', 'country', 'capitalism', 'obviously'], ['become', 'controversial', 'suggest', 'child', 'deserves', 'mother', 'father', 'deprived', 'child', 'mother', 'father', 'would', 'make', 'child', 'abuser', 'society', 'decided', 'mother', 'father', 'longer', 'word', 'said', 'many', 'kid', 'brought', 'family', 'mother', 'father', \"can't\", 'pretend'], ['create', 'oversensitive', 'society', \"there's\", 'premium', 'placed', 'offended', 'get', 'rewarded', 'offended', 'actually', 'creating', 'mental', 'illness', 'among', 'people', 'right', 'one', 'key', 'method', 'use', 'example', 'defeat', 'depression', 'trying', 'therapy', 'cognitive', 'behavioral', 'therapy', 'chain'], ['yesterday', 'senate', 'united', 'state', 'voted', 'push', 'forward', 'same-sex', 'marriage', 'legislation', 'calling', 'respect', 'marriage', 'act', 'absurd', 'respect', 'marriage', 'act', 'fundamentally', 'change', 'definition', 'word', 'marriage', 'respect', 'marriage', 'call', 'lot', 'thing', \"can't\", 'call'], ['favorite', 'argument', \"god's\", 'existence', 'favorite', 'argument', \"god's\", 'existence', 'believe', 'free', 'okay', 'reason', 'think', 'argument', \"god's\", 'existence', 'believe', 'human', 'being', 'essentially', 'ball', 'meat', 'wandering', 'around', 'aimlessly', 'universe', 'kind', 'spinosa', 'stone', 'think']]\n",
      "\n",
      "Found 583 videos for channels: \n",
      "  - Ben Shapiro\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "channelIds = [CHANNEL_MAPPINGS[c] for c in channels]\n",
    "texts = getTextsInDateRange(channelIds, start, stop, cleaned=True)\n",
    "channelstext = \"\\n  - \".join(channels)\n",
    "print([t[:30] for t in texts[:10]])\n",
    "print()\n",
    "print(f'Found {len(texts)} videos for channels: \\n  - {channelstext}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23b12a",
   "metadata": {},
   "source": [
    "# Build Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "553e0a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 583/583 [00:00<00:00, 759.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sad', 'left'), ('sad', 'california'), ('left', 'california'), ('left', 'sad'), ('left', 'tell'), ('california', 'tell'), ('california', 'left'), ('california', 'thought'), ('california', 'sad'), ('tell', 'thought')]\n",
      "[('boy', 'penis'), ('activity', 'year'), ('corporation', 'tech'), ('itself', 'way'), ('daily', 'copyright'), ('run', \"he'll\"), ('side', 'always'), ('abortion', 'concluded'), ('preventing', 'election'), ('and', 'want'), ('house', '41'), ('coveted', 'saying'), ('like', 'make'), ('leverage', 'additional'), ('anger', 'hatred'), ('republican', 'announces'), ('doubling', 'basically'), ('happily', \"he's\"), ('around', 'medical'), ('devil', 'satanist'), (\"there's\", 'like'), ('even', 'right'), ('amendment', 'constitutional'), ('hurt', 'dot'), ('staring', 'biden'), ('daca', 'right'), ('represented', 'descent'), ('poll', 'two'), ('already', 'actual'), ('safe', 'hand'), ('exactly', 'new'), ('ceiling', 'could'), ('assimilation', 'worried'), ('putin', 'vladimir'), ('red', 'red'), ('demonstrating', 'paper'), ('big', 'joe'), ('right', 'died'), ('trump', 'people'), ('continued', 'podcast'), ('already', 'made'), ('slower', 'soft'), ('good', 'west'), ('smack', 'get'), ('anything', 'saying'), ('civil', 'war'), ('response', 'left'), ('irresponsible', 'say'), ('month', 'extra'), ('perspective', 'judge'), ('success', 'based'), ('country', 'arab'), ('place', 'row'), ('forward', 'push'), ('hair', 'coromina'), ('abortion', 'historical'), ('resided', 'fact'), ('feel', 'like'), ('job', 'think'), ('repeat', 'force'), ('member', 'live'), ('inflation', 'one'), ('people', 'got'), ('west', 'fear'), ('place', 'ground'), ('city', 'living'), ('including', 'unquote'), ('employer', 'lot'), ('piece', \"there's\"), ('since', 'certainly'), ('mean', 'china'), ('really', 'big'), ('joe', 'legislative'), ('rise', 'right'), ('people', 'racism'), ('paying', 'fraction'), ('al-qaeda', 'gone'), ('reality', 'soar'), ('study', 'get'), ('official', 'according'), ('leader', 'china'), ('apparently', 'silly'), ('homosexual', 'superior'), (\"they're\", 'invested'), ('around', 'control'), ('right', 'activist'), ('professor', 'self-perpetuating'), ('wait', 'tomorrow'), ('driver', 'tank'), ('great', 'dinner'), ('bad', 'also'), ('genius', 'clap'), ('said', 'respond'), ('inflation', 'average'), (\"can't\", \"can't\"), ('transaction', 'gain'), ('people', 'investing'), ('yep', 'family'), ('travel', 'got'), ('eligible', 'immigrant')]\n",
      "Built 4492998 pairs\n"
     ]
    }
   ],
   "source": [
    "pairs = getPairs(texts, window=2)\n",
    "print(pairs[:10])\n",
    "print(random.sample(pairs, 100))\n",
    "print(f'Built {len(pairs)} pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958666e",
   "metadata": {},
   "source": [
    "# Get wordset and build tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663e2816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'crossover', 'upping', 'dismayed', 'trespasser', 'non-issue', 'houston', 'bluetooth', 'fargo', 'lib', 'absurdly', 'infuses', 'fleece', 'usps', '76.1', 'squanto', 'self-declared', 'effectuate', 'demonstrative', 'visor']\n",
      "['hampering', 'sherpa', 'eighteen', 'sotomayor', 'arkham', 'delicate', \"bond's\", 'staked', 'wrestler', 'hovering', 'istanbul', '1968', \"christ's\", 'catfishing', \"sock'em\", 'wear', 'debating', \"fascia's\", 'nationless', 'conflation']\n",
      "Wordset is size: 30493\n"
     ]
    }
   ],
   "source": [
    "wordset = getFullWordset(texts)\n",
    "word2idx, idx2word = buildTokenizer(wordset)\n",
    "print(list(wordset)[:20])\n",
    "print(random.sample(list(wordset), 20))\n",
    "print(f'Wordset is size: {len(wordset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a90f5",
   "metadata": {},
   "source": [
    "# Count pairs for creating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04be8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairWeights = {}\n",
    "for pair in pairs:\n",
    "    if pair not in pairWeights:\n",
    "        pairWeights[pair] = 0\n",
    "    pairWeights[pair] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb31415",
   "metadata": {},
   "source": [
    "# Convert Pairs to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98f0b499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26344, 12636), (71, 11521), (6064, 29736), (17539, 7186), (28974, 11433), (20220, 21700), (21250, 27427), (2642, 13738), (29056, 7183), (27688, 1967)]\n",
      "[11, 2, 1, 1, 1, 1, 1, 2, 2, 1]\n",
      "Total pairs: 1775754\n"
     ]
    }
   ],
   "source": [
    "def pairToInts(pair, mapping):\n",
    "    a, b = pair\n",
    "    return (mapping[a], mapping[b])\n",
    "\n",
    "pairset = set(pairs)\n",
    "weights = []\n",
    "intpairs = []\n",
    "# intpairs = [pairToInts(p, word2idx) for p in pairs]\n",
    "for p in pairset:\n",
    "    intpairs.append(pairToInts(p, word2idx))\n",
    "    weights.append(pairWeights[p])\n",
    "print(intpairs[:10])\n",
    "print(weights[:10])\n",
    "print(f'Total pairs: {len(intpairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d406e",
   "metadata": {},
   "source": [
    "# Convert ints to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53fd5baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30492\n",
      "0 30492\n",
      "30493\n"
     ]
    }
   ],
   "source": [
    "ins = [p[0] for p in intpairs]\n",
    "outs = [p[1] for p in intpairs]\n",
    "print(min(ins), max(ins))\n",
    "print(min(outs), max(outs))\n",
    "print(len(wordset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07adadc1",
   "metadata": {},
   "source": [
    "# Build X, Y matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "115e3d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1775754, 2)\n",
      "(1775754,)\n",
      "(1775754,)\n",
      "(1775754,)\n",
      "Final size will be: (1775754, 30493)\n",
      "Total points: 54,148,066,722\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "n = np.array(intpairs)\n",
    "a = n[:, 0]\n",
    "b = n[:, 1]\n",
    "trainingWeights = np.array(weights)\n",
    "print(n.shape)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(trainingWeights.shape)\n",
    "print(f'Final size will be: {len(n), len(wordset)}')\n",
    "print(f'Total points: {format(len(n)*len(wordset), \",d\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca055b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1775754\n",
      "1775754 1775754\n",
      "7.732770919799805 seconds\n",
      "(1775754, 30493)\n",
      "(1775754, 30493)\n"
     ]
    }
   ],
   "source": [
    "print(len(intpairs))\n",
    "print(len(a), len(b))\n",
    "start = time.time()\n",
    "\n",
    "X = np.zeros((a.size, a.max()+1), dtype=np.bool_)\n",
    "X[np.arange(a.size), a] = 1\n",
    "Y = np.zeros((b.size, b.max()+1), dtype=np.bool_)\n",
    "Y[np.arange(b.size), b] = 1\n",
    "    \n",
    "print(f'{time.time() - start} seconds')\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e72a9",
   "metadata": {},
   "source": [
    "# Build and compile NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d0f354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 64\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e86e4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(X.shape[1],))\n",
    "x = Dense(units=EMBED_SIZE, activation='linear')(inp)\n",
    "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a08be650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 30493)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                1951616   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 30493)             1982045   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,933,661\n",
      "Trainable params: 3,933,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d8cda",
   "metadata": {},
   "source": [
    "# Train NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a0ee90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9054ccf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingWeights\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    batch_size=32,\n",
    "    epochs=150,\n",
    "    sample_weight=trainingWeights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed6bc922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "# tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9becc4bf",
   "metadata": {},
   "source": [
    "# Performance Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb665bb4",
   "metadata": {},
   "source": [
    "# Build word -> vector mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()[0]\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "word2vec = {}\n",
    "for word in wordset:\n",
    "    vec = weights[word2idx[word]]\n",
    "    word2vec[word] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d454390",
   "metadata": {},
   "source": [
    "# Check pair counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pairs))\n",
    "print(len(set(pairs)))\n",
    "s = set(pairs)\n",
    "counts = {}\n",
    "for p in pairs:\n",
    "    if p not in counts:\n",
    "        counts[p] = 1\n",
    "    else:\n",
    "        counts[p] = counts[p]+1\n",
    "sortedpairs = sorted(list(s), key=lambda p: counts[p], reverse=True)\n",
    "for p in sortedpairs:\n",
    "    print(counts[p], p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a63c8a",
   "metadata": {},
   "source": [
    "# Get nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c96215",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'Biden'\n",
    "v = word2vec[targetWord]\n",
    "print(v.shape)\n",
    "\n",
    "dists = {}\n",
    "for word, vec in word2vec.items():\n",
    "    d = np.linalg.norm(v - vec)\n",
    "    dists[word] = d\n",
    "\n",
    "allwords = list(wordset)\n",
    "allwords = sorted(allwords, key=lambda w: dists[w])\n",
    "N = 10\n",
    "for i in range(N):\n",
    "    d = dists[allwords[i]]\n",
    "    print(f\"{i:<2} {str(round(d, 3)):<6} {allwords[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f5db8",
   "metadata": {},
   "source": [
    "# Export wordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f87618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtext = '\\n'.join(wordset)\n",
    "with open('wordset.txt', 'w') as file:\n",
    "    file.write(wordtext)\n",
    "    print('WROTE FILE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035685b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
